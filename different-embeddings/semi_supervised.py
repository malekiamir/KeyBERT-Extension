# -*- coding: utf-8 -*-
"""semi_supervised.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1xjirA3t7FCUMqX74hqoYLqu-hHlTmCh5
"""

!pip install keybert
!pip install keybert[flair]
!pip install keybert[gensim]
!pip install keybert[spacy]
!pip install keybert[use]
!pip install keybert sentence-transformers flair spacy gensim tensorflow-hub transformers
!python -m spacy download en_core_web_md
!pip install sklearn rouge_score
!pip install datasets
!pip install datasets
!pip install rouge_score

from keybert import KeyBERT
from sentence_transformers import SentenceTransformer
from flair.embeddings import TransformerDocumentEmbeddings
import spacy
import gensim.downloader as api
import tensorflow_hub as hub
import flair
import numpy as np
import torch
from transformers import AutoModelForSeq2SeqLM, AutoModel, AutoTokenizer, T5ForConditionalGeneration, BartForConditionalGeneration
from datasets import load_dataset
from rouge_score import rouge_scorer
from sklearn.metrics.pairwise import cosine_similarity
from sklearn.model_selection import train_test_split

#load Inspec dataset with trust_remote_code=True
ds = load_dataset("taln-ls2n/inspec", trust_remote_code=True)

texts = [sample["abstract"] for sample in ds["train"]]  #using 'abstract' as the main text
ground_truth_keywords = [sample["keyphrases"] for sample in ds["train"]]  # keyphrases' contains ground truth

# split dataset into train and test for semi-supervised learning
train_texts, test_texts, train_keywords, test_keywords = train_test_split(texts, ground_truth_keywords, test_size=0.3, random_state=42)

#train KeyBERT using train_texts
kw_model = KeyBERT()
train_results = {}
for text, gt_keywords in zip(train_texts, train_keywords):
    keywords = kw_model.extract_keywords(text, keyphrase_ngram_range=(1, 1), stop_words='english', top_n=5)
    train_results[text] = [kw[0] for kw in keywords]

#fine-tune KeyBERT using training data
refined_kw_model = KeyBERT()
refined_results = {}
for text in test_texts:
    refined_keywords = refined_kw_model.extract_keywords(text, keyphrase_ngram_range=(1, 1), stop_words='english', top_n=5)
    refined_results[text] = [kw[0] for kw in refined_keywords]

#evaluate the refined model using the test set
scorer = rouge_scorer.RougeScorer(["rouge1", "rouge2", "rougeL"], use_stemmer=True)
rouge_scores = {}
for text, gt_keywords in zip(test_texts, test_keywords):
    gt_text = " ".join(gt_keywords)
    pred_text = " ".join(refined_results[text])
    scores = scorer.score(gt_text, pred_text)
    rouge_scores[text] = {
        "ROUGE-1 F1": scores["rouge1"].fmeasure,
        "ROUGE-2 F1": scores["rouge2"].fmeasure,
        "ROUGE-L F1": scores["rougeL"].fmeasure,
    }

#compute average ROUGE scores
avg_rouge = {metric: np.mean([score[metric] for score in rouge_scores.values()]) for metric in ["ROUGE-1 F1", "ROUGE-2 F1", "ROUGE-L F1"]}

#print results
print("\nAverage ROUGE Scores for Refined Model:")
for metric, score in avg_rouge.items():
    print(f"  {metric}: {score:.4f}")

#compute cosine similarity between refined extracted keywords and ground truth
def get_embedding(model_name, text):
    model = SentenceTransformer("all-MiniLM-L6-v2")
    return model.encode([text])[0]

cosine_scores = {}
for text, gt_keywords in zip(test_texts, test_keywords):
    if not gt_keywords or not refined_results[text]:  
        continue

    gt_text = " ".join(gt_keywords)
    pred_text = " ".join(refined_results[text])

    gt_embedding = get_embedding("sentence-transformers", gt_text)
    pred_embedding = get_embedding("sentence-transformers", pred_text)

    similarity = cosine_similarity([gt_embedding], [pred_embedding])[0][0]
    cosine_scores[text] = similarity

#compute average Cosine Similarity
avg_cosine_similarity = np.mean(list(cosine_scores.values()))

print("\nAverage Cosine Similarity for Refined Model:", round(avg_cosine_similarity, 4))